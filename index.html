<!DOCTYPE html>
<html><head>
<style>
body {
  margin-top: 50px;
  margin-bottom: 50px;
  margin-left: 80px;
  margin-right: 80px;
  background-image: linear-gradient(rgba(255, 255, 255, 0.5), rgba(255, 255, 255, 0.5)), url('figures/background.jpg');
  background-repeat: no-repeat;
  background-attachment: fixed;
  background-size: 100% 100%;
}
p { 
  text-align: justify;
  font-size: 1em;
  line-height: 1.4;
}

p.padding {
  padding-left: 10px;
}

ul { 
  text-align: justify;
  font-size: 1em;
  line-height: 1.4;
}

a:link {
  color:rgb(158, 3, 3);
  background-color: transparent;
  text-decoration: none;
}

hl {
  color:rgb(158, 3, 3);
  background-color: transparent;
  text-decoration: none;
}

hl2 {
  color: rgb(158, 3, 3);
  background-color: transparent;
  text-decoration: underline;
}

a:visited {
  color: rgb(158, 3, 3);
  background-color: transparent;
  text-decoration: none;
}

li {
    margin-bottom: 8px;
    font-size: 1em;
    line-height: 1.4;
}

.section {max-width:1024px}
</style>
<meta name="msvalidate.01" content="F09EB454EEBF59AD68CB64C1A341AA9A" />
<meta name="baidu-site-verification" content="Ctys5yXPkU" />
<meta name="google-site-verification" content="9qnP6wIE8rh3TwMVrb84fWPqg9A_nDYEL3D6NGDjpwU" />
<meta name="keywords" content="Zengyi Qin, Zen Qin, Charles Qin, Charles Z. Qin">
<title>Zengyi Qin | MIT</title>
<meta charset="utf-8">

<script async defer src="https://buttons.github.io/buttons.js"></script>
</head>

<body text="#444444" link="#444444" leftmargin="40" bgcolor="#FFFFFF">

<div class="section">
<table width="100%">
<tr>
<td width="20%">
<p align="center"><font face="Verdana"><img src="figures/zengyi.jpg" border="0" width="200"></font></p>
</td>
<td width="3%"></td>
<td width="72%">
<h2><font face="Verdana">Zengyi Qin</h2>
<p><font face="Verdana">qinzy [at] mit.edu</font></p>
</td>
</tr>
</table>
</div>

<div class="section">
<p><font face="Verdana">
  I am a PhD student at MIT, affiliated with MIT CSAIL and AeroAstro. My experience spans <hl>foundation models (language, audio, etc), 3D computer vision, and robotics.</hl> My ultimate goal is to develop <hl>general-purpose intelligence</hl> that advances how machines <hl>understand, interact, and create</hl> in both the digital and physical worlds. I am fortunate to work with <hl>Dr. Brian Anthony</hl> and <hl>Bill Freeman</hl> at MIT.
</font></p>
<p><font face="Verdana">
  I also have extensive experience <hl>converting cutting-edge research into practical products</hl>. I co-founded <a href="www.myshell.ai">MyShell.ai</a> and developed the agentic framework for everyone to build AI agents without coding. The platform now has >3M users and more than 10K apps are built.
</font></p>
</div>

<div class="section">
<h3><font face="Verdana">Education</h3>
<p><font face="Verdana">
<ul>
<li><b>Massachusetts Institute of Technology</b>, 2020 - present <br> Doctor of Philosophy - PhD <br> Affiliated with <a href="https://www.csail.mit.edu/">MIT CSAIL</a> (Computer Science and Artificial Intelligence Laboratory) and <a href="https://aeroastro.mit.edu/">MIT AeroAstro</a>. </li>
<li><b>Massachusetts Institute of Technology</b>, 2020 - 2022. Master of Science <br>
<li><b>Tsinghua University</b>, 2016 - 2020. Bachelor of Engineering, Electronic Engineering<br>
</ul>
</font></p>
</div>

<div class="section">
  <h3><font face="Verdana">News</h3>
  <p><font face="Verdana">
   <ul>
    <li>
      [Apr 2024] We released <hl>JetMoE-8B</hl>, which was trained from scratch with less than $0.1M cost but outperforms LLaMA2-7B. It went viral and received strong positive feedback from the field.
      <ul>
        <a href="https://research.myshell.ai/jetmoe">Technical blog</a> <br> 
        <a href="https://x.com/MIT_CSAIL/status/1775916496503656679">MIT CSAIL posts</a> <br> 
        Comments from the field (<a href="https://www.marktechpost.com/2024/04/05/myshell-ai-and-mit-researchers-propose-jetmoe-8b-a-super-efficient-llm-model-that-achieves-llama2-level-training-with-just-us-0-1m/">1</a> <a href="https://www.linkedin.com/posts/pfinette_jetmoe-reaching-llama2-performance-with-activity-7182038641882357760-2QIh/">2</a> <a href="https://x.com/natolambert/status/1776028430406123992">3</a>) <br>
        <i>The breakthrough represented by JetMoE-8B signals a significant democratization of AI technology </i> (<a href="https://www.marktechpost.com/2024/04/05/myshell-ai-and-mit-researchers-propose-jetmoe-8b-a-super-efficient-llm-model-that-achieves-llama2-level-training-with-just-us-0-1m/">1</a>)
      </ul>
    </li>
    <li>
      [Jan 2024] We released <a href="https://research.myshell.ai/open-voice">OpenVoice</a>, allowing users to clone any voice and generate speech in various styles and languages.
      <ul>
        <a href="https://research.myshell.ai/open-voice">Technical blog</a> <br> 
        Trended<a href="./figures/githubtrending-no1-v3.PNG"> 1st on Github</a>. Now 27k stars <br> 
        <hl>Serving >3M users</hl> on MyShell. Solid production-grade algorithm <br>
        Covered by <a href='https://venturebeat.com/ai/open-source-ai-voice-cloning-arrives-with-myshells-new-openvoice-model/'>VentureBeat</a>, <a href="https://hyscaler.com/insights/ai-voice-cloning-revolution/">HyScaler</a> and other medias <br>
        <i>AI Voice Cloning Redefined: OpenVoice Unveils Revolutionary Open-Source Technology</i> (<a href="https://hyscaler.com/insights/ai-voice-cloning-revolution">1</a>)
      </ul>
    </li>
   </ul>
  </font></p>
</div>

<div class="section">
  <h3 id="publications"><font face="Verdana">Projects in Generative Models</h3>
  <table width="100%">

  <!-- JetMoE -->
  <tr>
    <td width="25%" valign="middle"><p><img src="figures/jetmoe.PNG" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b><a id="openvoice">JetMoE: Reaching LLaMA2 Performance with 0.1M Dollars</a></b><br>
    Yikang Shen, Zhen Guo, Tianle Cai and <b>Zengyi Qin</b><br>
    <i>Technical Report, 2024</i><br>
    </p>
    <p class="padding"><font face="Verdana"> Training high-performance LLM with remarkable cost efficiency. <br>
    <a href="https://research.myshell.ai/jetmoe">website</a> | <a href="https://github.com/myshell-ai/JetMoE">github</a> | <a href="https://arxiv.org/pdf/2404.07413.pdf">tech report</a> <br>
    </p>
    </td>
  </tr>

  <!-- OpenVoice -->
  <tr>
    <td width="25%" valign="middle"><p><img src="figures/openvoice.png" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b><a id="openvoice">OpenVoice: Versatile Instant Voice Cloning</a></b><br>
    <b>Zengyi Qin</b>, Wenliang Zhao, Xumin Yu and Xin Sun<br>
    <i>Technical Report, 2024</i><br>
    </p>
    <p class="padding"><font face="Verdana"> Instantly clone any voice to generate speech in various styles and languages. <br>
    <a href="https://arxiv.org/abs/2312.01479">paper</a> | <a href="https://research.myshell.ai/open-voice">website</a> | <a href="https://github.com/myshell-ai/OpenVoice">source code</a> <br>
    </p>
    <p class="padding">
    <hl>Trended 1st on Github</hl> <a class="github-button" href="https://github.com/myshell-ai/OpenVoice" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star myshell-ai/OpenVoice on GitHub">Star</a> 
    </p></td>
  </tr>

  <!-- Interspeech -->
  <tr>
    <td width="25%" valign="middle"><p><img src="figures/dreamvoice.png" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="middle">
    <p class="padding"><font face="Verdana">
    <b><a id="dreamvoice">DreamVoice: Text-Guided Voice Conversion</a></b><br>
    Jiarui Hai, Karan Thakkar, Helin Wang, <b>Zengyi Qin</b>, Mounya Elhilali<br>
    <i>Interspeech, 2024</i><br>
    </p>
    <p class="padding"><font face="Verdana"> Convert a voice into any voice based on the input text prompt. <br>
    <a href="https://arxiv.org/abs/2406.16314">paper</a> | <a href="https://research.myshell.ai/dreamvoice">website</a> | <a href="https://github.com/myshell-ai/DreamVoice">source code</a> <br>
    </p>
  </tr>

  <!-- MeloTTS -->
  <tr>
    <td width="25%" valign="middle"><p><img src="figures/melotts.png" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="middle">
    <p class="padding"><font face="Verdana">
    <b><a id="melotts">MeloTTS: A high-quality multi-lingual multi-accent text-to-speech library</a></b><br>
    Wenliang Zhao, Xumin Yu, <b>Zengyi Qin</b>
    </p>
    <p class="padding"><font face="Verdana"> High-quality multi-lingual text-to-speech library that supports English (US, BR, AU, INDIAN), Spanish, French, Chinese, Japanese and Korean <br>
    <a href="https://github.com/myshell-ai/MeloTTS">source code</a> <br>
    </p>
    <p class="padding">
      <a class="github-button" href="https://github.com/myshell-ai/MeloTTS" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star myshell-ai/OpenVoice on GitHub">Star</a> 
    </p></td>
  </tr>

</table>
</div> 


<div class="section">
  <h3 id="publications"><font face="Verdana">Projects in 3D Computer Vision</h3>
  <table width="100%">

  <!-- MONOGRNET-TPAMI -->
  <tr>
    <td width="25%" valign="top"><p><img src="figures/monogrnet_pami.PNG" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b><a id="monogrnet">MonoGRNet: A General Framework for Monocular 3D Object Detection</a></b><br>
    <b>Zengyi Qin</b>, Jinglu Wang and Yan Lu<br>
    <i>The IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2021</i><br>
    </p>
    <p class="padding"><font face="Verdana">A general monocular 3D object detection framework that flexibly adapts to both fully and weakly supervised learning, which alleviates the need of extensive 3D labels and only requires ground truth 2D bounding boxes during training.
    </p>
    <p class="padding"><font face="Verdana">
    <a href="https://ieeexplore.ieee.org/document/9409679">paper</a>
    </p></td>
  </tr>

  <!-- VS3D -->
  <tr>
    <td width="25%" valign="top"><p><img src="figures/vs3d.gif" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b>Weakly Supervised 3D Object Detection from Point Clouds</b><br>
    <b>Zengyi Qin</b>, Jinglu Wang and Yan Lu<br>
    <i>ACM Multimedia (<b>ACM MM</b>), 2020</i><br>
    </p>
    <p class="padding"><font face="Verdana">A state-of-the-art framework for weakly supervised 3D object detection from point clouds without using any ground truth 3D bounding box for training. The core of our method is the unsupervised 3D object proposal module and the cross-modal knowledge distillation strategy.
    </p>
    <p class="padding"><font face="Verdana">
    <a href="https://arxiv.org/abs/2007.13970">paper</a> | <a href="https://github.com/Zengyi-Qin/Weakly-Supervised-3D-Object-Detection">code</a>
    </p></td>
  </tr>

  <!-- TLNET -->
  <tr>
    <td width="25%" valign="top"><p><img src="figures/tlnet.jpg" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b>Triangulation Learning Network: from Monocular to Stereo 3D Object Detection</b><br>
    <b>Zengyi Qin</b>, Jinglu Wang and Yan Lu<br>
    <i>The International Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019</i><br>
    </p>
    <p class="padding"><font face="Verdana">This is a pioneering work on stereo image based 3D object detection without calculating the pixel-level depth maps. We proposed a triangulation learning method to learn the object-level stereo geometric correspondence for 3D object detection.
    </p>
    <p class="padding"><font face="Verdana">
    <a href="https://arxiv.org/abs/1906.01193">paper</a> | <a href="https://youtu.be/zcOXA83Uq8M">video</a> | <a href="https://github.com/Zengyi-Qin/TLNet">code</a> | <a href="https://sites.google.com/view/triangulation-learning">website</a>
    </p></td>
  </tr>
    
    <!-- MONOGRNET -->
  <tr>
    <td width="25%" valign="top"><p><img src="figures/monogrnet.gif" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b>MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization</b><br>
    <b>Zengyi Qin</b>, Jinglu Wang and Yan Lu<br>
    <i>The Thirty-Third AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2019, <b>Oral Presentation, Acceptance Rate < 8% </b> </i><br>
    </p>
    <p class="padding"><font face="Verdana">A state-of-the-art monocular 3D object detection approach based on geometric reasoning. We proposed to decompose the whole task into four progressive sub-tasks that significantly facilitates the monocular 3D object detection.
    </p>
    <p class="padding"><font face="Verdana">
    <a href="https://arxiv.org/abs/1811.10247">paper</a> | <a href="https://youtu.be/BJ1I56kxdk0">video</a> | <a href="https://github.com/Zengyi-Qin/MonoGRNet">code</a> | <a href="https://sites.google.com/view/monogrnet">website</a>
    </p></td>
  </tr>

</table>
</div> 

  <div class="section">
  <h3 id="publications"><font face="Verdana">Projects in Robotics</h3>
  <table width="100%">

  <!-- Sablas -->
  <tr>
    <td width="25%" valign="top"><p><video autoplay="" class="center" loop="" muted="" playsinline="" style="display:block; margin: 0 auto;" width="250"><source src="figures/sablas.mp4" style="display: block; margin-left: auto; margin-right: auto; width:250px" type="video/mp4" /></video></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b><a id="sablas">SABLAS: Learning Safe Control for Black-Box Dynamical Systems</a></b><br>
    <b>Zengyi Qin</b>, Dawei Sun and Chuchu Fan<br>
    <i>IEEE Robotics and Automation Letters (<b>RA-L</b>), 2022</i><br>
    </p>
    <p class="padding"><font face="Verdana"> Learning control barrier functions (CBFs) for safe control of black-box systems. CBFs are a powerful tool to provide safety guarantee, but before this work, they cannot be directly applied to black box systems where their models are unavailable.
    </p>
    <p class="padding"><font face="Verdana">
    <a href="https://ieeexplore.ieee.org/document/9681233">paper</a> | <a href="https://github.com/MIT-REALM/sablas">code</a>
    </p></td>
  </tr>


  <!-- KETO -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/keto.gif" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b><a id="keto">KETO: Learning Keypoint Representations for Tool Manipulation</a></b><br>
  <b>Zengyi Qin</b>, Kuan Fang, Yuke Zhu, Li Fei-Fei and Silvio Savarese<br>
  <i>The International Conference on Robotics and Automation (<b>ICRA</b>), 2020</i><br>
  </p>
  <p class="padding"><font face="Verdana">KETO is a framework for robots to manipulate unseen objects as tools to complete diverse tasks. We proposed a method to learn the keypoint representations of objects, which simplify the manipulation task and improve the generality to novel objects.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://arxiv.org/abs/1910.11977">paper</a> | <a href="https://www.youtube.com/watch?v=hP2h53BHxE8">video</a> | <a href="https://sites.google.com/view/ke-to">website</a> | <a href="https://github.com/StanfordVL/keto">code</a>
  </p></td>
  </tr>
    
  <!-- MACBF -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/macbf.gif" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b><a id="macbf">Learning Safe Multi-agent Control with Decentralized Neural Barrier Certificates</b><br>
  <b>Zengyi Qin</b>, Kaiqing Zhang, Yuxiao Chen, Jingkai Chen and Chuchu Fan<br>
  <i>The International Conference on Learning Representations (<b>ICLR</b>), 2021</i><br>
  </p>
  <p class="padding"><font face="Verdana">We study the multi-agent safe control problem where agents should avoid any collision while reaching their goals. Our method can scale up to an arbitrarily large number of agents (e.g., >1000 in our experiments) and achieve a 99-100% safety rate.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://arxiv.org/abs/2101.05436">paper</a> | <a href="https://www.youtube.com/watch?v=k9ap_aUPNBk">video</a> | <a href="https://github.com/Zengyi-Qin/macbf">code</a> | <a href="https://realm.mit.edu/blog/learning-safe-multi-agent-control-decentralized-neural-barrier-certificates">website</a>
  </p></td>
  </tr>
   
  <!-- Reactive -->
  <tr>
  <td width="25%" valign="top"><p><video autoplay="" class="center" loop="" muted="" playsinline="" style="display:block; margin: 0 auto;" width="250"><source src="figures/reactive.mp4" style="display: block; margin-left: auto; margin-right: auto; width:250px" type="video/mp4" /></video></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b><a id="reactive">Reactive and Safe Road User Simulations using Neural Barrier Certificate</a></b><br>
  Yue Meng, <b>Zengyi Qin</b> and Chuchu Fan<br>
  <i>The International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2021</i><br>
  </p>
  <p class="padding"><font face="Verdana">Reactive and safe agent modelings are important for nowadays traffic simulator designs and safe planning applications. We propose a control barrier function-based method to simulate traffic agents that behave like humans or human controlled vehicles, which react to other road participants.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="">paper</a> | <a href="https://realm.mit.edu/reactive-agent-models">website</a>
  </p></td>
  </tr>
    
  <!-- DCRL -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/dcrl.png" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b><a id="dcrl">Density Constrained Reinforcement Learning</a></b><br>
  <b>Zengyi Qin</b>, Yuxiao Chen and Chuchu Fan<br>
  <i>The International Conference on Machine Learning (<b>ICML</b>), 2021</i><br>
  </p>
  <p class="padding"><font face="Verdana">We study constrained reinforcement learning (CRL) from a novel perspective by setting constraints directly on state density functions, rather than the value functions considered by previous work. State density has a clear physical and mathematical interpretation, and is able to express a wide variety of constraints such as resource limits and safety requirements.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://arxiv.org/abs/2106.12764">paper</a> | <a href="https://github.com/Zengyi-Qin/dcrl">code</a> | <a href="http://realm.mit.edu/blog/density-constrained-reinforcement-learning">website</a>
  </p></td>
  </tr>
  
  <!-- CLBF -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/clbf.JPG" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b><a id="clbf">Safe Nonlinear Control Using Robust Neural Lyapunov-Barrier Functions</a></b><br>
  Charles Dawson, <b>Zengyi Qin</b>, Sicun Gao and Chuchu Fan<br>
  <i>The Conference on Robot Learning (<b>CoRL</b>), 2021</i><br>
  </p>
  <p class="padding"><font face="Verdana">Safety and stability are common requirements for robotic control systems. We propose a robust feedback method based on robust control Lyapunov barrier functions that generalize despite model uncertainty, and with safety and stability guarantee.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://arxiv.org/pdf/2109.06697">paper</a>
  </p></td>
  </tr>
  
  <!-- REALSYN -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/controller_synthesis.PNG" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b>Controller synthesis for linear system with reach-avoid specifications</b><br>
  Chuchu Fan, <b>Zengyi Qin</b>, Umang Mathur, Qiang Ning, Sayan Mitra, and Mahesh Viswanathan<br>
  <i>IEEE Transactions on Automatic Control (<b>TAC</b>), 2021</i><br>
  </p>
  <p class="padding"><font face="Verdana">We address the problem of synthesizing provably correct controllers for linear systems with reach-avoid specifications. Our solution  decomposes the overall synthesis problem into two smaller and more tractable problems, achieving a 2-150 times speedup compared with the previous techniques.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://ieeexplore.ieee.org/document/9390193">paper</a>
  </p></td>
  </tr>

</table>
</div> 
  
  <div class="section">
    <h3 id="publications"><font face="Verdana">Projects in AI for Healthcare</h3>
    <table width="100%">
  
  <!-- PDoctor -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/pdoctor.PNG" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b>Learning fine-grained estimation of physiological states from coarse-grained labels by distribution restoration</b><br>
  <b>Zengyi Qin</b>, Jiansheng Chen, Zhenyu Jiang, Xumin Yu, Chunhua Hu, Yu Ma, Suhua Miao and Rongsong Zhou, <i> <b>Scientific Reports</b>, 2020</i><br>
  </p>
  <p class="padding"><font face="Verdana">Our method allows machine learning algorithms to perform fine-grained estimation of physiological states (e.g., sleep depth) even if the training labels are coarse-grained.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://www.nature.com/articles/s41598-020-79007-5">paper</a> | <a href="https://github.com/Zengyi-Qin/fine-biostate">code</a>
  </p></td>
  </tr>
  
  <!-- SNET -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/snet.png" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b>sEMG based Tremor Severity Evaluation for Parkinson's Disease using a Light-weight CNN</b><br>
  <b>Zengyi Qin<top>*</top></b>, Zhenyu Jiang<top>*</top>, Jiansheng Chen, Chunhua Hu and Yu Ma<br>
  <i>IEEE Signal Processing Letters (<b>SPL</b>), 2019</i><br>
  </p>
  <p class="padding"><font face="Verdana">A machine learning framework to assist the diagnosis of Parkinson's Disease by assessing the pathological tremor. We proposed a light-weight convolutional neural network and a similarity learning strategy to handle the scarcity of medical data.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://ieeexplore.ieee.org/document/8661631">paper</a> | <a href="https://sites.google.com/view/semg-tremor">website</a>
  </p></td>
  </tr>
  
  </table>
</div> 

</body></html>
