<!DOCTYPE html>
<html><head>
<style>
body {
  margin-top: 50px;
  margin-bottom: 50px;
  margin-left: 80px;
  margin-right: 80px;
  background-image: linear-gradient(rgba(255, 255, 255, 0.5), rgba(255, 255, 255, 0.5)), url('figures/background.jpg');
  background-repeat: no-repeat;
  background-attachment: fixed;
  background-size: 100% 100%;
}
p { 
  text-align: justify;
  font-size: 1em;
  line-height: 1.4;
}

p.padding {
  padding-left: 10px;
}

ul { 
  text-align: justify;
  font-size: 1em;
  line-height: 1.4;
}

a:link {
  color:rgb(130, 3, 3);
  background-color: transparent;
  text-decoration: none;
}

hl {
  color:rgb(130, 3, 3);
  background-color: transparent;
  text-decoration: none;
}

hl2 {
  color: rgb(130, 3, 3);
  background-color: transparent;
  text-decoration: underline;
}

a:visited {
  color: rgb(130, 3, 3);
  background-color: transparent;
  text-decoration: none;
}

li {
    margin-bottom: 8px;
    font-size: 1em;
    line-height: 1.4;
}

.section {max-width:1024px}
</style>
<meta name="msvalidate.01" content="F09EB454EEBF59AD68CB64C1A341AA9A" />
<meta name="baidu-site-verification" content="Ctys5yXPkU" />
<meta name="google-site-verification" content="9qnP6wIE8rh3TwMVrb84fWPqg9A_nDYEL3D6NGDjpwU" />
<meta name="keywords" content="Zengyi Qin">
<meta property="og:image" content="figures/zengyi.jpg">
<meta property="og:image:width" content="256">
<meta property="og:image:height" content="256">
<meta property="og:image:type" content="image/jpeg">
<meta property="og:title" content="Zengyi Qin">
<meta property="og:description" content="MIT PhD | GenAI">
<title>Zengyi Qin</title>
<meta charset="utf-8">

<script async defer src="https://buttons.github.io/buttons.js"></script>
</head>

<body text="#444444" link="#444444" leftmargin="40" bgcolor="#FFFFFF">

<div class="section">
<table width="100%">
<tr>
<td width="20%">
<p align="center"><font face="Verdana"><img src="figures/zengyi.jpg" border="0" width="200"></font></p>
</td>
<td width="3%"></td>
<td width="72%">
<h2><font face="Verdana">Zengyi Qin</h2>
<p><font face="Verdana">qinzy [at] alum.mit.edu</font></p>
<a href="https://twitter.com/mitqinzy?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @qinzytech</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</td>
</tr>
</table>
</div>

<div class="section">
<p><font face="Verdana">
  Researcher and entrepreneur (company public listed). Currently cooking a <hl>BIG</hl> thing.
</font></p>
<p><font face="Verdana">
  Had a wonderful PhD journey at <a href="https://www.csail.mit.edu">MIT</a>. Was the primary author of several widely recognized AI models. One has <a href="https://github.com/myshell-ai/OpenVoice">~30k stars</a> and was <a href="https://www.qinzy.tech/figures/githubtrending-no1-v3.PNG">trending 1st</a> on Github. It ranked <a href="https://github.com/myshell-ai/OpenVoice">top 0.03%</a> of all open-source projects created during the similar period, based on popularity. Another project <a href="https://trendshift.io/repositories/8133">also trended 1st</a> on GitHub, receives <a href="https://www.qinzy.tech/figures/total_download.PNG">19.2M downloads</a>, making it one of the most widely adopted AI models since 2024.
</font></p>
<p><font face="Verdana">
  Have solid experience pre-training and post-training LLMs. Project lead of <a href="https://research.myshell.ai/jetmoe">JetMoE-8B</a>, an MoA+MoE LLM pre-trained and post-trained <a href="https://research.myshell.ai/jetmoe">from scratch</a> (not finetuning existing models), under <a href="https://research.myshell.ai/jetmoe">extreme limitations in compute and data</a>, with <a href="https://research.myshell.ai/jetmoe">less than 0.1M USD cost</a>, but outperforms LLaMA2-7B.
</font></p>
<p><font face="Verdana">
  Co-founded an <a href="https://www.myshell.ai">AI Agent platform MyShell</a> for everyone to orchestrate hundreds of AI models to build and use agentic applications. The platform now has <a href="https://www.myshell.ai">> 5 million users</a> and <a href="https://www.myshell.ai">>200,000 AI agents</a> are built. The users have had <a href="https://www.myshell.ai">> 1 billion interactions</a> with the AI agents. These make it one of the most successful agent platforms in the field.
</font></p>
</div>

<div class="section">
<h3><font face="Verdana">Education</h3>
<p><font face="Verdana">
<ul>
<li><b>Massachusetts Institute of Technology</b>, PhD, <a href="https://www.csail.mit.edu/">MIT</a></li>
<li><b>Stanford University</b>, Visiting Researcher in <a href="https://svl.stanford.edu/">Stanford Vision and Learning Lab</a> <br>
<li><b>Tsinghua University</b>, BE, Electronic Engineering<br>
</ul>
</font></p>
</div>

<div class="section">
  <h3><font face="Verdana">News</h3>
  <p><font face="Verdana">
   <ul>
    <li>
      [Feb 2025] MyShell is publicly listed on major crypto exchanges. Huge congrats to the team! This is a really hard but rewarding journey because almost nobody believed AI + Decentralization make sense when we started MyShell two years ago. But now it already becomes a consensus.
      <ul>
        Media coverage: <a href="https://www.binance.com/en/square/post/20859989647258">Binance</a>, <a href="https://www.panewslab.com/en/articledetails/9f748vm9.html">PANews</a>, <a href="https://x.com/binance/status/1894985212679037293">Twitter</a><br>
      </ul>
    </li>
    <li>
      [Apr 2024] I lead the development of <a href="https://research.myshell.ai/jetmoe">JetMoE-8B</a>, an MoA+MoE LLM pre-trained and post-trained from scratch, under extreme limitations in compute and data, with less than 0.1M USD cost, but outperforms LLaMA2-7B. It democratized high-performance LLM pre-training and post-training, and received strong positive feedback from the field.
      <ul>
        <a href="https://research.myshell.ai/jetmoe">Technical blog</a> <br> 
        <a href="https://x.com/MIT_CSAIL/status/1775916496503656679">MIT CSAIL posts</a> <br> 
        Comments from the field (<a href="https://www.marktechpost.com/2024/04/05/myshell-ai-and-mit-researchers-propose-jetmoe-8b-a-super-efficient-llm-model-that-achieves-llama2-level-training-with-just-us-0-1m/">1</a> <a href="https://www.linkedin.com/posts/pfinette_jetmoe-reaching-llama2-performance-with-activity-7182038641882357760-2QIh/">2</a> <a href="https://x.com/natolambert/status/1776028430406123992">3</a>) <br>
        <i>The breakthrough represented by JetMoE-8B signals a significant democratization of AI technology </i> (<a href="https://www.marktechpost.com/2024/04/05/myshell-ai-and-mit-researchers-propose-jetmoe-8b-a-super-efficient-llm-model-that-achieves-llama2-level-training-with-just-us-0-1m/">1</a>)
      </ul>
    </li>
    <li>
      [Jan 2024] I lead the development of <a href="https://research.myshell.ai/open-voice">OpenVoice</a>, an audio foundation model that allows users to clone any voice and generate speech in various styles and languages.
      <ul>
        <a href="https://research.myshell.ai/open-voice">Technical blog</a> <br> 
        Trended<a href="./figures/githubtrending-no1-v3.PNG"> 1st on Github</a> <br>
        Now <hl>~30k stars</hl>, and ranked <hl>top 0.03%</hl> of all contemporary open-source projects based on stars <br> 
        <hl>Serving >3M users</hl> on MyShell. Solid production-grade algorithm <br>
        Covered by <a href='https://venturebeat.com/ai/open-source-ai-voice-cloning-arrives-with-myshells-new-openvoice-model/'>VentureBeat</a>, <a href="https://hyscaler.com/insights/ai-voice-cloning-revolution/">HyScaler</a> and other medias <br>
        <i>AI Voice Cloning Redefined: OpenVoice Unveils Revolutionary Open-Source Technology</i> (<a href="https://hyscaler.com/insights/ai-voice-cloning-revolution">1</a>) <br>
      </ul>
    </li>
   </ul>
  </font></p>
</div>

<div class="section">
  <h3 id="genai"><font face="Verdana">Projects in Generative Models</h3>
  <table width="100%">

  <!-- JetMoE -->
  <tr>
    <td width="25%" valign="middle"><p><img src="figures/jetmoe.PNG" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b><a id="openvoice">JetMoE: Reaching LLaMA2 Performance with 0.1M Dollars</a></b><br>
    Yikang Shen, Zhen Guo, Tianle Cai and <b>Zengyi Qin</b><br>
    <i>Technical Report, 2024</i><br>
    </p>
    <p class="padding"><font face="Verdana"> JetMoE is pre-trained and post-trained from scratch with less than 0.1M USD cost but outperforms LLaMA2-7B. It democratized high-performance LLM pre-training and post-training with remarkable cost-efficiency. <br>
    <a href="https://research.myshell.ai/jetmoe">website</a> | <a href="https://github.com/myshell-ai/JetMoE">github</a> | <a href="https://arxiv.org/pdf/2404.07413.pdf">tech report</a> <br>
    </p>
    </td>
  </tr>

  <!-- OpenVoice -->
  <tr>
    <td width="25%" valign="middle"><p><img src="figures/openvoice.png" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b><a id="openvoice">OpenVoice: Versatile Instant Voice Cloning</a></b><br>
    <b>Zengyi Qin</b>, Wenliang Zhao, Xumin Yu and Xin Sun<br>
    <i>Technical Report, 2024</i><br>
    </p>
    <p class="padding"><font face="Verdana"> Instantly clone any voice to generate speech in various styles and languages. <br>
    <a href="https://arxiv.org/abs/2312.01479">paper</a> | <a href="https://research.myshell.ai/open-voice">website</a> | <a href="https://github.com/myshell-ai/OpenVoice">source code</a> <br>
    </p>
    <p class="padding">
    <a class="github-button" href="https://github.com/myshell-ai/OpenVoice" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star myshell-ai/OpenVoice on GitHub">Star</a> <br>
    <a href="https://trendshift.io/repositories/6161" target="_blank"><img src="https://trendshift.io/api/badge/repositories/6161" alt="myshell-ai%2FOpenVoice | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>  
  </p></td>
  </tr>

  <!-- MeloTTS -->
  <tr>
    <td width="25%" valign="middle"><p><img src="figures/melotts.png" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="middle">
    <p class="padding"><font face="Verdana">
    <b><a id="melotts">MeloTTS: A high-quality multi-lingual multi-accent text-to-speech library</a></b><br>
    Wenliang Zhao, Xumin Yu, <b>Zengyi Qin</b>
    </p>
    <p class="padding"><font face="Verdana"> High-quality multi-lingual text-to-speech library that supports English (US, BR, AU, INDIAN), Spanish, French, Chinese, Japanese and Korean <br>
    <a href="https://github.com/myshell-ai/MeloTTS">source code</a> <br>
    </p>
    <p class="padding">
      <a class="github-button" href="https://github.com/myshell-ai/MeloTTS" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star myshell-ai/OpenVoice on GitHub">Star</a> <hl>19.2M downloads. One of the most widely adopted AI models since 2024</hl> <br>
      <a href="https://trendshift.io/repositories/8133" target="_blank"><img src="https://trendshift.io/api/badge/repositories/8133" alt="myshell-ai%2FMeloTTS | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
    </p></td>
  </tr>

    <!-- Interspeech -->
  <tr>
    <td width="25%" valign="middle"><p><img src="figures/dreamvoice.png" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="middle">
    <p class="padding"><font face="Verdana">
    <b><a id="dreamvoice">DreamVoice: Text-Guided Voice Conversion</a></b><br>
    Jiarui Hai, Karan Thakkar, Helin Wang, <b>Zengyi Qin</b>, Mounya Elhilali<br>
    <i>Interspeech, 2024</i><br>
    </p>
    <p class="padding"><font face="Verdana"> Convert a voice into any voice based on the input text prompt. <br>
    <a href="https://arxiv.org/abs/2406.16314">paper</a> | <a href="https://research.myshell.ai/dreamvoice">website</a> | <a href="https://github.com/myshell-ai/DreamVoice">source code</a> <br>
    </p>
  </tr>

</table>
</div> 


<div class="section">
  <h3 id="publications"><font face="Verdana">Projects in 3D Computer Vision</h3>
  <table width="100%">

  <!-- MONOGRNET-TPAMI -->
  <tr>
    <td width="25%" valign="top"><p><img src="figures/monogrnet_pami.PNG" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b><a id="monogrnet">MonoGRNet: A General Framework for Monocular 3D Object Detection</a></b><br>
    <b>Zengyi Qin</b>, Jinglu Wang and Yan Lu<br>
    <i>The IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2021</i><br>
    </p>
    <p class="padding"><font face="Verdana">A general monocular 3D object detection framework that flexibly adapts to both fully and weakly supervised learning, which alleviates the need of extensive 3D labels and only requires ground truth 2D bounding boxes during training.
    </p>
    <p class="padding"><font face="Verdana">
    <a href="https://ieeexplore.ieee.org/document/9409679">paper</a>
    </p></td>
  </tr>

  <!-- VS3D -->
  <tr>
    <td width="25%" valign="top"><p><img src="figures/vs3d.gif" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b>Weakly Supervised 3D Object Detection from Point Clouds</b><br>
    <b>Zengyi Qin</b>, Jinglu Wang and Yan Lu<br>
    <i>ACM Multimedia (<b>ACM MM</b>), 2020</i><br>
    </p>
    <p class="padding"><font face="Verdana">A state-of-the-art framework for weakly supervised 3D object detection from point clouds without using any ground truth 3D bounding box for training. The core of our method is the unsupervised 3D object proposal module and the cross-modal knowledge distillation strategy.
    </p>
    <p class="padding"><font face="Verdana">
    <a href="https://arxiv.org/abs/2007.13970">paper</a> | <a href="https://github.com/Zengyi-Qin/Weakly-Supervised-3D-Object-Detection">code</a>
    </p></td>
  </tr>

  <!-- TLNET -->
  <tr>
    <td width="25%" valign="top"><p><img src="figures/tlnet.jpg" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b>Triangulation Learning Network: from Monocular to Stereo 3D Object Detection</b><br>
    <b>Zengyi Qin</b>, Jinglu Wang and Yan Lu<br>
    <i>The International Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019</i><br>
    </p>
    <p class="padding"><font face="Verdana">This is a pioneering work on stereo image based 3D object detection without calculating the pixel-level depth maps. We proposed a triangulation learning method to learn the object-level stereo geometric correspondence for 3D object detection.
    </p>
    <p class="padding"><font face="Verdana">
    <a href="https://arxiv.org/abs/1906.01193">paper</a> | <a href="https://youtu.be/zcOXA83Uq8M">video</a> | <a href="https://github.com/Zengyi-Qin/TLNet">code</a> | <a href="https://sites.google.com/view/triangulation-learning">website</a>
    </p></td>
  </tr>
    
    <!-- MONOGRNET -->
  <tr>
    <td width="25%" valign="top"><p><img src="figures/monogrnet.gif" alt="" style="border-style: none" width="250" align="top"></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b>MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization</b><br>
    <b>Zengyi Qin</b>, Jinglu Wang and Yan Lu<br>
    <i>The Thirty-Third AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2019 <br> <hl>Oral Presentation, Acceptance Rate < 8% </hl> </i><br>
    </p>
    <p class="padding"><font face="Verdana">A state-of-the-art monocular 3D object detection approach based on geometric reasoning. We proposed to decompose the whole task into four progressive sub-tasks that significantly facilitates the monocular 3D object detection.
    </p>
    <p class="padding"><font face="Verdana">
    <a href="https://arxiv.org/abs/1811.10247">paper</a> | <a href="https://youtu.be/BJ1I56kxdk0">video</a> | <a href="https://github.com/Zengyi-Qin/MonoGRNet">code</a> | <a href="https://sites.google.com/view/monogrnet">website</a>
    </p></td>
  </tr>

</table>
</div> 

  <div class="section">
  <h3 id="publications"><font face="Verdana">Projects in Reinforcement Learning and Robotics</h3>
  <table width="100%">

  <!-- Sablas -->
  <tr>
    <td width="25%" valign="top"><p><video autoplay="" class="center" loop="" muted="" playsinline="" style="display:block; margin: 0 auto;" width="250"><source src="figures/sablas.mp4" style="display: block; margin-left: auto; margin-right: auto; width:250px" type="video/mp4" /></video></p></td>
    <td width="75%" valign="top">
    <p class="padding"><font face="Verdana">
    <b><a id="sablas">SABLAS: Learning Safe Control for Black-Box Dynamical Systems</a></b><br>
    <b>Zengyi Qin</b>, Dawei Sun and Chuchu Fan<br>
    <i>IEEE Robotics and Automation Letters (<b>RA-L</b>), 2022</i><br>
    </p>
    <p class="padding"><font face="Verdana"> Learning control barrier functions (CBFs) for safe control of black-box systems. CBFs are a powerful tool to provide safety guarantee, but before this work, they cannot be directly applied to black box systems where their models are unavailable.
    </p>
    <p class="padding"><font face="Verdana">
    <a href="https://ieeexplore.ieee.org/document/9681233">paper</a> | <a href="https://github.com/MIT-REALM/sablas">code</a>
    </p></td>
  </tr>


  <!-- KETO -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/keto.gif" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b><a id="keto">KETO: Learning Keypoint Representations for Tool Manipulation</a></b><br>
  <b>Zengyi Qin</b>, Kuan Fang, Yuke Zhu, Li Fei-Fei and Silvio Savarese<br>
  <i>The International Conference on Robotics and Automation (<b>ICRA</b>), 2020</i><br>
  </p>
  <p class="padding"><font face="Verdana">KETO is a framework for robots to manipulate unseen objects as tools to complete diverse tasks. We proposed a method to learn the keypoint representations of objects, which simplify the manipulation task and improve the generality to novel objects.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://arxiv.org/abs/1910.11977">paper</a> | <a href="https://www.youtube.com/watch?v=hP2h53BHxE8">video</a> | <a href="https://sites.google.com/view/ke-to">website</a> | <a href="https://github.com/StanfordVL/keto">code</a>
  </p></td>
  </tr>
    
  <!-- MACBF -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/macbf.gif" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b><a id="macbf">Learning Safe Multi-agent Control with Decentralized Neural Barrier Certificates</b><br>
  <b>Zengyi Qin</b>, Kaiqing Zhang, Yuxiao Chen, Jingkai Chen and Chuchu Fan<br>
  <i>The International Conference on Learning Representations (<b>ICLR</b>), 2021</i><br>
  </p>
  <p class="padding"><font face="Verdana">We study the multi-agent safe control problem where agents should avoid any collision while reaching their goals. Our method can scale up to an arbitrarily large number of agents (e.g., >1000 in our experiments) and achieve a 99-100% safety rate.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://arxiv.org/abs/2101.05436">paper</a> | <a href="https://www.youtube.com/watch?v=k9ap_aUPNBk">video</a> | <a href="https://github.com/Zengyi-Qin/macbf">code</a> | <a href="https://realm.mit.edu/blog/learning-safe-multi-agent-control-decentralized-neural-barrier-certificates">website</a>
  </p></td>
  </tr>
   
  <!-- Reactive -->
  <tr>
  <td width="25%" valign="top"><p><video autoplay="" class="center" loop="" muted="" playsinline="" style="display:block; margin: 0 auto;" width="250"><source src="figures/reactive.mp4" style="display: block; margin-left: auto; margin-right: auto; width:250px" type="video/mp4" /></video></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b><a id="reactive">Reactive and Safe Road User Simulations using Neural Barrier Certificate</a></b><br>
  Yue Meng, <b>Zengyi Qin</b> and Chuchu Fan<br>
  <i>The International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2021</i><br>
  </p>
  <p class="padding"><font face="Verdana">Reactive and safe agent modelings are important for nowadays traffic simulator designs and safe planning applications. We propose a control barrier function-based method to simulate traffic agents that behave like humans or human controlled vehicles, which react to other road participants.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="">paper</a> | <a href="https://realm.mit.edu/reactive-agent-models">website</a>
  </p></td>
  </tr>
    
  <!-- DCRL -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/dcrl.png" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b><a id="dcrl">Density Constrained Reinforcement Learning</a></b><br>
  <b>Zengyi Qin</b>, Yuxiao Chen and Chuchu Fan<br>
  <i>The International Conference on Machine Learning (<b>ICML</b>), 2021</i><br>
  </p>
  <p class="padding"><font face="Verdana">We study constrained reinforcement learning (CRL) from a novel perspective by setting constraints directly on state density functions, rather than the value functions considered by previous work. State density has a clear physical and mathematical interpretation, and is able to express a wide variety of constraints such as resource limits and safety requirements.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://arxiv.org/abs/2106.12764">paper</a> | <a href="https://github.com/Zengyi-Qin/dcrl">code</a> | <a href="http://realm.mit.edu/blog/density-constrained-reinforcement-learning">website</a>
  </p></td>
  </tr>
  
  <!-- CLBF -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/clbf.JPG" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b><a id="clbf">Safe Nonlinear Control Using Robust Neural Lyapunov-Barrier Functions</a></b><br>
  Charles Dawson, <b>Zengyi Qin</b>, Sicun Gao and Chuchu Fan<br>
  <i>The Conference on Robot Learning (<b>CoRL</b>), 2021</i><br>
  </p>
  <p class="padding"><font face="Verdana">Safety and stability are common requirements for robotic control systems. We propose a robust feedback method based on robust control Lyapunov barrier functions that generalize despite model uncertainty, and with safety and stability guarantee.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://arxiv.org/pdf/2109.06697">paper</a>
  </p></td>
  </tr>
  
  <!-- REALSYN -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/controller_synthesis.PNG" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b>Controller synthesis for linear system with reach-avoid specifications</b><br>
  Chuchu Fan, <b>Zengyi Qin</b>, Umang Mathur, Qiang Ning, Sayan Mitra, and Mahesh Viswanathan<br>
  <i>IEEE Transactions on Automatic Control (<b>TAC</b>), 2021</i><br>
  </p>
  <p class="padding"><font face="Verdana">We address the problem of synthesizing provably correct controllers for linear systems with reach-avoid specifications. Our solution  decomposes the overall synthesis problem into two smaller and more tractable problems, achieving a 2-150 times speedup compared with the previous techniques.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://ieeexplore.ieee.org/document/9390193">paper</a>
  </p></td>
  </tr>

</table>
</div> 
  
  <div class="section">
    <h3 id="publications"><font face="Verdana">Projects in AI for Healthcare</h3>
    <table width="100%">
  
  <!-- PDoctor -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/pdoctor.PNG" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b>Learning fine-grained estimation of physiological states from coarse-grained labels by distribution restoration</b><br>
  <b>Zengyi Qin</b>, Jiansheng Chen, Zhenyu Jiang, Xumin Yu, Chunhua Hu, Yu Ma, Suhua Miao and Rongsong Zhou, <i> <b>Scientific Reports</b>, 2020</i><br>
  </p>
  <p class="padding"><font face="Verdana">Our method allows machine learning algorithms to perform fine-grained estimation of physiological states (e.g., sleep depth) even if the training labels are coarse-grained.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://www.nature.com/articles/s41598-020-79007-5">paper</a> | <a href="https://github.com/Zengyi-Qin/fine-biostate">code</a>
  </p></td>
  </tr>
  
  <!-- SNET -->
  <tr>
  <td width="25%" valign="top"><p><img src="figures/snet.png" alt="" style="border-style: none" width="250" align="top"></p></td>
  <td width="75%" valign="top">
  <p class="padding"><font face="Verdana">
  <b>sEMG based Tremor Severity Evaluation for Parkinson's Disease using a Light-weight CNN</b><br>
  <b>Zengyi Qin<top>*</top></b>, Zhenyu Jiang<top>*</top>, Jiansheng Chen, Chunhua Hu and Yu Ma<br>
  <i>IEEE Signal Processing Letters (<b>SPL</b>), 2019</i><br>
  </p>
  <p class="padding"><font face="Verdana">A machine learning framework to assist the diagnosis of Parkinson's Disease by assessing the pathological tremor. We proposed a light-weight convolutional neural network and a similarity learning strategy to handle the scarcity of medical data.
  </p>
  <p class="padding"><font face="Verdana">
  <a href="https://ieeexplore.ieee.org/document/8661631">paper</a> | <a href="https://sites.google.com/view/semg-tremor">website</a>
  </p></td>
  </tr>
  
  </table>
</div> 

<script>
  // Function to fetch FDV from CoinGecko API and update the page
  async function fetchFDV() {
    try {
      const response = await fetch(
        'https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd&ids=myshell'
      );
      if (!response.ok) {
        throw new Error(`API request failed (status ${response.status})`);
      }
      const data = await response.json();
      // Ensure data format is as expected and contains FDV
      if (data && data[0] && data[0].fully_diluted_valuation) {
        const fdvValue = data[0].fully_diluted_valuation;
        // Format the FDV with commas and prefix with '$'
        document.getElementById('fdv1').textContent = 
          '$' + Number(fdvValue).toLocaleString();
        document.getElementById('fdv2').textContent = 
          '$' + Number(fdvValue).toLocaleString();
      } else {
        document.getElementById('fdv1').textContent = 'NA';
        document.getElementById('fdv2').textContent = 'NA';
      }
    } catch (error) {
      console.error('Error fetching FDV:', error);
      document.getElementById('fdv').textContent = 'NA';
    }
  }

  // Fetch FDV immediately and then update every 60 seconds
  fetchFDV();
  setInterval(fetchFDV, 60000);
</script>

</body></html>
